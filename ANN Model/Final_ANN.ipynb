{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_2JbO6uzjaMg"
   },
   "outputs": [],
   "source": [
    "# Importing relevant modules\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.neural_network\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "# Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bMI9KGEknlY",
    "outputId": "015fbdd7-49a9-4fe6-8480-5ce2fd503a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X1     X2   X3     X4   X5      X6     X7     X8    y\n",
      "0  540.0    0.0  0.0  162.0  2.5  1040.0  676.0   28.0  1.0\n",
      "1  540.0    0.0  0.0  162.0  2.5  1055.0  676.0   28.0  1.0\n",
      "2  332.5  142.5  0.0  228.0  0.0   932.0  594.0  270.0  1.0\n",
      "3  332.5  142.5  0.0  228.0  0.0   932.0  594.0  365.0  1.0\n",
      "\n",
      "Dataset has 1030 rows and 9 columns\n",
      "\n",
      "0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "3    1.0\n",
      "Name: y, dtype: float64\n",
      "(1030,)\n",
      "\n",
      "      X1     X2   X3     X4   X5      X6     X7     X8\n",
      "0  540.0    0.0  0.0  162.0  2.5  1040.0  676.0   28.0\n",
      "1  540.0    0.0  0.0  162.0  2.5  1055.0  676.0   28.0\n",
      "2  332.5  142.5  0.0  228.0  0.0   932.0  594.0  270.0\n",
      "3  332.5  142.5  0.0  228.0  0.0   932.0  594.0  365.0\n",
      "(1030, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset:\n",
    "data_url = 'https://raw.githubusercontent.com/Moataz-AbdElKhalek/Concrete_Compressive_Strength_Prediction/main/dataset/Concrete_Dataset_Classification.csv'\n",
    "dataset = pd.read_csv(data_url)\n",
    "\n",
    "print(dataset.head(4))\n",
    "\n",
    "# Descriptive statistics\n",
    "print(\"\\nDataset has {} rows and {} columns\".format(dataset.shape[0],dataset.shape[1]))\n",
    "\n",
    "print()\n",
    "y = dataset['y']\n",
    "print(y.head(4))\n",
    "print(y.shape)\n",
    "print()\n",
    "\n",
    "X = dataset.drop(['y'], axis=1)\n",
    "print(X.head(4))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "2-M8wgu1ks0J",
    "outputId": "d69c4ee6-90f1-4197-ebff-349235baccf4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.895825</td>\n",
       "      <td>54.188350</td>\n",
       "      <td>181.567282</td>\n",
       "      <td>6.204660</td>\n",
       "      <td>972.918932</td>\n",
       "      <td>773.580485</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>-0.048544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.279342</td>\n",
       "      <td>63.997004</td>\n",
       "      <td>21.354219</td>\n",
       "      <td>5.973841</td>\n",
       "      <td>77.753954</td>\n",
       "      <td>80.175980</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>0.999306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.300000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                X1           X2           X3           X4           X5  \\\n",
       "count  1030.000000  1030.000000  1030.000000  1030.000000  1030.000000   \n",
       "mean    281.167864    73.895825    54.188350   181.567282     6.204660   \n",
       "std     104.506364    86.279342    63.997004    21.354219     5.973841   \n",
       "min     102.000000     0.000000     0.000000   121.800000     0.000000   \n",
       "25%     192.375000     0.000000     0.000000   164.900000     0.000000   \n",
       "50%     272.900000    22.000000     0.000000   185.000000     6.400000   \n",
       "75%     350.000000   142.950000   118.300000   192.000000    10.200000   \n",
       "max     540.000000   359.400000   200.100000   247.000000    32.200000   \n",
       "\n",
       "                X6           X7           X8            y  \n",
       "count  1030.000000  1030.000000  1030.000000  1030.000000  \n",
       "mean    972.918932   773.580485    45.662136    -0.048544  \n",
       "std      77.753954    80.175980    63.169912     0.999306  \n",
       "min     801.000000   594.000000     1.000000    -1.000000  \n",
       "25%     932.000000   730.950000     7.000000    -1.000000  \n",
       "50%     968.000000   779.500000    28.000000    -1.000000  \n",
       "75%    1029.400000   824.000000    56.000000     1.000000  \n",
       "max    1145.000000   992.600000   365.000000     1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying statistical Analysis on the data:\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHeidtrkvqbe"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "mXYIhCUkvpsj",
    "outputId": "06c5d49f-4612-4ed1-a1a2-d97801254e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Attributes:\n",
      " Index(['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8'], dtype='object')\n",
      "\n",
      "Before Data Scaling:\n",
      "       X1     X2   X3     X4   X5      X6     X7     X8\n",
      "0  540.0    0.0  0.0  162.0  2.5  1040.0  676.0   28.0\n",
      "1  540.0    0.0  0.0  162.0  2.5  1055.0  676.0   28.0\n",
      "2  332.5  142.5  0.0  228.0  0.0   932.0  594.0  270.0\n",
      "3  332.5  142.5  0.0  228.0  0.0   932.0  594.0  365.0\n",
      "\n",
      "After Data Scaling:\n",
      "          X1        X2   X3        X4       X5        X6       X7        X8\n",
      "0  1.000000 -1.000000 -1.0 -0.357827 -0.84472  0.389535 -0.58856 -0.851648\n",
      "1  1.000000 -1.000000 -1.0 -0.357827 -0.84472  0.476744 -0.58856 -0.851648\n",
      "2  0.052511 -0.207012 -1.0  0.696486 -1.00000 -0.238372 -1.00000  0.478022\n",
      "3  0.052511 -0.207012 -1.0  0.696486 -1.00000 -0.238372 -1.00000  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.181882</td>\n",
       "      <td>-0.588782</td>\n",
       "      <td>-0.458387</td>\n",
       "      <td>-0.045251</td>\n",
       "      <td>-0.614617</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>-0.098944</td>\n",
       "      <td>-0.754604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.477198</td>\n",
       "      <td>0.480130</td>\n",
       "      <td>0.639650</td>\n",
       "      <td>0.341122</td>\n",
       "      <td>0.371046</td>\n",
       "      <td>0.452058</td>\n",
       "      <td>0.402288</td>\n",
       "      <td>0.347087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.587329</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.311502</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.238372</td>\n",
       "      <td>-0.312845</td>\n",
       "      <td>-0.967033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.219635</td>\n",
       "      <td>-0.877574</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>-0.602484</td>\n",
       "      <td>-0.029070</td>\n",
       "      <td>-0.069242</td>\n",
       "      <td>-0.851648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.132420</td>\n",
       "      <td>-0.204508</td>\n",
       "      <td>0.182409</td>\n",
       "      <td>0.121406</td>\n",
       "      <td>-0.366460</td>\n",
       "      <td>0.327907</td>\n",
       "      <td>0.154039</td>\n",
       "      <td>-0.697802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                X1           X2           X3           X4           X5  \\\n",
       "count  1030.000000  1030.000000  1030.000000  1030.000000  1030.000000   \n",
       "mean     -0.181882    -0.588782    -0.458387    -0.045251    -0.614617   \n",
       "std       0.477198     0.480130     0.639650     0.341122     0.371046   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.587329    -1.000000    -1.000000    -0.311502    -1.000000   \n",
       "50%      -0.219635    -0.877574    -1.000000     0.009585    -0.602484   \n",
       "75%       0.132420    -0.204508     0.182409     0.121406    -0.366460   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                X6           X7           X8  \n",
       "count  1030.000000  1030.000000  1030.000000  \n",
       "mean     -0.000471    -0.098944    -0.754604  \n",
       "std       0.452058     0.402288     0.347087  \n",
       "min      -1.000000    -1.000000    -1.000000  \n",
       "25%      -0.238372    -0.312845    -0.967033  \n",
       "50%      -0.029070    -0.069242    -0.851648  \n",
       "75%       0.327907     0.154039    -0.697802  \n",
       "max       1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Scikit-learn MaxMinScaler: \n",
    "scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# extract attributes and scale data to have Min = -1 and Max = 1 :\n",
    "cols = X.columns\n",
    "print('Data Attributes:\\n', cols)\n",
    "print('\\nBefore Data Scaling:\\n', X.head(4))\n",
    "sc_X = scaler.fit_transform(X) # Fit scaler to data, then transform data to specified feature_range(-1,1)\n",
    "\n",
    "# Turn the scaling results back into a dataframe :\n",
    "sc_X_df = pd.DataFrame(sc_X, columns = cols)\n",
    "X = sc_X_df\n",
    "print('\\nAfter Data Scaling:\\n', X.head(4))\n",
    "\n",
    "# Applying statistical Analysis on the data:\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrrCjXj8tr7L"
   },
   "source": [
    "# **Artificial Neural Network (ANN) Model Optimization**\n",
    "# Using 10-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fOlRo1Mpldal"
   },
   "outputs": [],
   "source": [
    "def ANN_CV(max_iterations,alpha_range, hidden_layers):\n",
    "    \n",
    "  # Preparing the Model:\n",
    "  model = sklearn.neural_network.MLPClassifier(activation='relu',random_state=1)\n",
    "  \n",
    "  # Determining Model Hyperparameters to be tested and optimized:\n",
    "  paras = {'max_iter':max_iterations, 'alpha':alpha_range, 'hidden_layer_sizes':hidden_layers}\n",
    "\n",
    "  # Preparing Cross-Validation to be used to fit the Model and the Hyperparameters:\n",
    "  # Using 10-fold Cross-Validation:\n",
    "  gridCV = sklearn.model_selection.GridSearchCV(model, paras, cv=10, scoring='accuracy', verbose=10, n_jobs = -1)\n",
    "  gridCV.fit(X, y)\n",
    "\n",
    "  best_max_iterations = gridCV.best_params_['max_iter']\n",
    "  best_alpha = gridCV.best_params_['alpha']\n",
    "  best_hidden_layers = gridCV.best_params_['hidden_layer_sizes']\n",
    "  best_score = gridCV.best_score_\n",
    "  results = gridCV.cv_results_\n",
    "\n",
    "  return best_max_iterations, best_alpha, best_hidden_layers, best_score, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EclwloLCqcwd"
   },
   "outputs": [],
   "source": [
    "# Using the ranges with max score in the case of fixed split: \n",
    "test_max_iter_range = numpy.arange(50,1000,200) \n",
    "test_alpha_range = [.1, .01, 1e-3, 1e-4 , 1e-5]\n",
    "layers = []\n",
    "number_of_layers = 4\n",
    "number_of_nodes_per_layers = [4, 8, 12]\n",
    "\n",
    "for n in number_of_nodes_per_layers:\n",
    "  temp = numpy.array([[n]])\n",
    "  for m in range(number_of_layers):\n",
    "    hidden = numpy.repeat(temp, repeats=m+1, axis=1)\n",
    "    layers.append(hidden[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8TqQ48blhK3",
    "outputId": "614213d2-fb77-4294-95ff-29f99b4a7f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 300 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   50.3s\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 529 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 669 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 745 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 825 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 909 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1042 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1185 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1285 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1336 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1389 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1609 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1666 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1725 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1845 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1906 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2162 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2229 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2296 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2365 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2505 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2649 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2722 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2872 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2949 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 15.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Testing the ANN Model using Cross-Validation with Grid Search to determine best score (accuracy) and most optimum Hyperparameters:\n",
    "best_max_iterations, best_alpha, best_hidden_layers, best_score, results = ANN_CV(test_max_iter_range, test_alpha_range, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ECEfV0oOljXF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_max_iterations = 650\n",
      "best_alpha = 0.01\n",
      "best_hidden_layers = [12, 12, 12]\n",
      "Cross-Validation Mean Best Score for the Model = 0.8242718446601941\n",
      "\n",
      "Cross-Validation Mean Test Scores\n",
      " [0.69320388 0.7407767  0.76796117 0.76699029 0.76796117 0.58737864\n",
      " 0.7815534  0.79805825 0.80776699 0.81941748 0.60582524 0.75533981\n",
      " 0.78543689 0.79417476 0.79320388 0.65436893 0.8038835  0.81553398\n",
      " 0.81262136 0.81359223 0.65145631 0.73786408 0.76019417 0.78640777\n",
      " 0.79417476 0.72427184 0.80194175 0.78737864 0.78932039 0.79126214\n",
      " 0.6961165  0.80679612 0.79902913 0.79902913 0.79805825 0.68834951\n",
      " 0.80776699 0.79320388 0.79126214 0.79029126 0.65145631 0.75631068\n",
      " 0.78446602 0.78932039 0.77572816 0.72135922 0.78543689 0.78252427\n",
      " 0.80776699 0.81262136 0.73203883 0.81165049 0.81456311 0.80970874\n",
      " 0.80776699 0.73203883 0.77572816 0.78058252 0.7815534  0.7815534\n",
      " 0.69320388 0.73883495 0.76893204 0.77087379 0.77281553 0.58640777\n",
      " 0.77961165 0.80776699 0.8038835  0.80776699 0.60679612 0.75631068\n",
      " 0.7776699  0.79417476 0.79514563 0.65728155 0.80291262 0.80194175\n",
      " 0.81262136 0.8184466  0.65242718 0.73398058 0.76116505 0.78640777\n",
      " 0.79708738 0.72330097 0.8        0.78252427 0.78543689 0.78932039\n",
      " 0.69805825 0.77669903 0.7776699  0.78932039 0.79223301 0.70970874\n",
      " 0.79902913 0.80291262 0.79708738 0.79514563 0.65048544 0.75339806\n",
      " 0.78543689 0.77572816 0.76796117 0.7223301  0.78737864 0.79126214\n",
      " 0.79514563 0.80679612 0.73495146 0.80582524 0.81456311 0.82427184\n",
      " 0.8223301  0.73009709 0.78058252 0.78058252 0.7815534  0.7815534\n",
      " 0.69320388 0.73883495 0.76893204 0.76990291 0.76990291 0.58640777\n",
      " 0.77961165 0.80679612 0.80776699 0.80776699 0.60485437 0.75631068\n",
      " 0.78349515 0.79126214 0.79805825 0.65631068 0.80679612 0.81359223\n",
      " 0.81553398 0.81941748 0.65242718 0.73300971 0.76116505 0.78446602\n",
      " 0.79417476 0.72427184 0.80097087 0.78737864 0.79320388 0.79126214\n",
      " 0.70097087 0.77961165 0.77864078 0.79223301 0.80194175 0.70970874\n",
      " 0.80873786 0.79902913 0.79805825 0.79126214 0.65048544 0.75339806\n",
      " 0.78640777 0.77281553 0.76796117 0.7223301  0.78737864 0.78737864\n",
      " 0.79902913 0.81359223 0.73495146 0.8038835  0.80970874 0.81067961\n",
      " 0.81165049 0.72718447 0.75728155 0.76893204 0.77864078 0.7776699\n",
      " 0.69320388 0.73883495 0.76893204 0.76990291 0.76990291 0.58737864\n",
      " 0.77961165 0.80679612 0.80970874 0.80776699 0.60485437 0.75631068\n",
      " 0.77961165 0.78349515 0.78446602 0.65242718 0.8038835  0.80291262\n",
      " 0.80194175 0.8038835  0.65242718 0.73203883 0.76116505 0.78446602\n",
      " 0.79417476 0.72427184 0.8038835  0.77475728 0.78932039 0.78543689\n",
      " 0.69417476 0.7815534  0.78349515 0.79320388 0.79708738 0.70776699\n",
      " 0.80291262 0.80194175 0.79514563 0.80582524 0.65048544 0.75339806\n",
      " 0.78640777 0.77475728 0.77087379 0.72135922 0.78640777 0.78446602\n",
      " 0.7961165  0.8038835  0.73786408 0.80873786 0.81359223 0.8184466\n",
      " 0.81650485 0.72718447 0.7592233  0.76407767 0.76407767 0.76213592\n",
      " 0.69320388 0.73883495 0.76893204 0.77087379 0.77087379 0.58737864\n",
      " 0.77961165 0.80582524 0.80776699 0.80679612 0.60582524 0.74951456\n",
      " 0.7776699  0.78737864 0.8        0.65631068 0.80582524 0.80194175\n",
      " 0.8        0.80970874 0.65242718 0.73203883 0.76116505 0.78446602\n",
      " 0.79514563 0.72330097 0.7961165  0.77669903 0.78543689 0.79029126\n",
      " 0.70097087 0.79223301 0.79223301 0.80776699 0.81067961 0.70582524\n",
      " 0.8        0.79417476 0.78737864 0.7776699  0.65048544 0.75145631\n",
      " 0.78737864 0.77475728 0.76893204 0.7223301  0.78834951 0.78543689\n",
      " 0.80097087 0.81067961 0.7368932  0.80776699 0.81650485 0.82330097\n",
      " 0.82330097 0.72912621 0.77281553 0.78446602 0.78252427 0.78252427]\n",
      "\n",
      "Split_1 Scores\n",
      " [0.62135922 0.65048544 0.66990291 0.66990291 0.66990291 0.38834951\n",
      " 0.65048544 0.66990291 0.70873786 0.72815534 0.52427184 0.66990291\n",
      " 0.70873786 0.7184466  0.7184466  0.67961165 0.70873786 0.69902913\n",
      " 0.66019417 0.66019417 0.59223301 0.65048544 0.66019417 0.70873786\n",
      " 0.70873786 0.63106796 0.7184466  0.66990291 0.66019417 0.67961165\n",
      " 0.53398058 0.7184466  0.68932039 0.67961165 0.68932039 0.55339806\n",
      " 0.73786408 0.70873786 0.70873786 0.70873786 0.66990291 0.67961165\n",
      " 0.68932039 0.67961165 0.70873786 0.63106796 0.65048544 0.66990291\n",
      " 0.7184466  0.72815534 0.66990291 0.7184466  0.69902913 0.69902913\n",
      " 0.69902913 0.62135922 0.66990291 0.66990291 0.66990291 0.66990291\n",
      " 0.62135922 0.65048544 0.66990291 0.66990291 0.66990291 0.38834951\n",
      " 0.65048544 0.68932039 0.65048544 0.65048544 0.51456311 0.66990291\n",
      " 0.67961165 0.70873786 0.70873786 0.67961165 0.7184466  0.72815534\n",
      " 0.73786408 0.73786408 0.59223301 0.65048544 0.66990291 0.7184466\n",
      " 0.7184466  0.63106796 0.7184466  0.66990291 0.65048544 0.65048544\n",
      " 0.55339806 0.69902913 0.66019417 0.66990291 0.66990291 0.60194175\n",
      " 0.69902913 0.69902913 0.69902913 0.69902913 0.66990291 0.67961165\n",
      " 0.69902913 0.67961165 0.67961165 0.63106796 0.65048544 0.69902913\n",
      " 0.72815534 0.7184466  0.66019417 0.72815534 0.7184466  0.7184466\n",
      " 0.7184466  0.62135922 0.61165049 0.63106796 0.63106796 0.63106796\n",
      " 0.62135922 0.65048544 0.66990291 0.66990291 0.66990291 0.38834951\n",
      " 0.65048544 0.68932039 0.67961165 0.66019417 0.51456311 0.65048544\n",
      " 0.66019417 0.7184466  0.7184466  0.67961165 0.7184466  0.72815534\n",
      " 0.72815534 0.72815534 0.59223301 0.65048544 0.66990291 0.7184466\n",
      " 0.7184466  0.63106796 0.7184466  0.65048544 0.65048544 0.65048544\n",
      " 0.55339806 0.68932039 0.67961165 0.67961165 0.65048544 0.60194175\n",
      " 0.67961165 0.67961165 0.67961165 0.67961165 0.66990291 0.67961165\n",
      " 0.69902913 0.67961165 0.68932039 0.63106796 0.65048544 0.69902913\n",
      " 0.7184466  0.73786408 0.66990291 0.70873786 0.70873786 0.68932039\n",
      " 0.68932039 0.62135922 0.60194175 0.60194175 0.60194175 0.60194175\n",
      " 0.62135922 0.65048544 0.66990291 0.66990291 0.66990291 0.38834951\n",
      " 0.65048544 0.68932039 0.68932039 0.66019417 0.51456311 0.66019417\n",
      " 0.66019417 0.7184466  0.72815534 0.66019417 0.7184466  0.72815534\n",
      " 0.73786408 0.73786408 0.59223301 0.65048544 0.66990291 0.7184466\n",
      " 0.7184466  0.63106796 0.72815534 0.65048544 0.65048544 0.65048544\n",
      " 0.53398058 0.70873786 0.67961165 0.67961165 0.67961165 0.61165049\n",
      " 0.69902913 0.68932039 0.68932039 0.68932039 0.66990291 0.67961165\n",
      " 0.69902913 0.68932039 0.69902913 0.63106796 0.65048544 0.68932039\n",
      " 0.7184466  0.73786408 0.67961165 0.70873786 0.72815534 0.72815534\n",
      " 0.72815534 0.62135922 0.58252427 0.60194175 0.60194175 0.60194175\n",
      " 0.62135922 0.65048544 0.66990291 0.66990291 0.66990291 0.38834951\n",
      " 0.65048544 0.68932039 0.66990291 0.66019417 0.51456311 0.66019417\n",
      " 0.66019417 0.7184466  0.7184466  0.66019417 0.7184466  0.70873786\n",
      " 0.72815534 0.72815534 0.59223301 0.65048544 0.66990291 0.7184466\n",
      " 0.7184466  0.63106796 0.7184466  0.6407767  0.63106796 0.6407767\n",
      " 0.53398058 0.7184466  0.67961165 0.67961165 0.67961165 0.61165049\n",
      " 0.68932039 0.67961165 0.67961165 0.67961165 0.66990291 0.67961165\n",
      " 0.69902913 0.67961165 0.68932039 0.63106796 0.65048544 0.68932039\n",
      " 0.7184466  0.72815534 0.66990291 0.7184466  0.69902913 0.69902913\n",
      " 0.69902913 0.62135922 0.60194175 0.59223301 0.59223301 0.59223301]\n",
      "best_score (Split_1) = 0.7378640776699029\n",
      "\n",
      "Split_2 Scores\n",
      " [0.93203883 0.9223301  0.9223301  0.9223301  0.9223301  0.65048544\n",
      " 0.91262136 0.81553398 0.81553398 0.80582524 0.74757282 0.9223301\n",
      " 0.90291262 0.81553398 0.75728155 0.82524272 0.88349515 0.85436893\n",
      " 0.85436893 0.85436893 0.70873786 0.95145631 0.93203883 0.91262136\n",
      " 0.89320388 0.94174757 0.87378641 0.80582524 0.72815534 0.67961165\n",
      " 0.98058252 0.76699029 0.66990291 0.66990291 0.66990291 0.95145631\n",
      " 0.77669903 0.68932039 0.68932039 0.68932039 0.68932039 0.93203883\n",
      " 0.9223301  0.88349515 0.72815534 0.98058252 0.90291262 0.69902913\n",
      " 0.68932039 0.69902913 0.91262136 0.72815534 0.72815534 0.72815534\n",
      " 0.72815534 0.94174757 0.66019417 0.65048544 0.65048544 0.65048544\n",
      " 0.93203883 0.9223301  0.9223301  0.9223301  0.9223301  0.65048544\n",
      " 0.91262136 0.84466019 0.81553398 0.80582524 0.74757282 0.9223301\n",
      " 0.88349515 0.86407767 0.86407767 0.82524272 0.86407767 0.80582524\n",
      " 0.72815534 0.72815534 0.70873786 0.95145631 0.93203883 0.91262136\n",
      " 0.88349515 0.93203883 0.87378641 0.77669903 0.72815534 0.72815534\n",
      " 0.98058252 0.67961165 0.66019417 0.66019417 0.66019417 0.95145631\n",
      " 0.73786408 0.67961165 0.67961165 0.67961165 0.68932039 0.93203883\n",
      " 0.9223301  0.81553398 0.7184466  0.98058252 0.90291262 0.73786408\n",
      " 0.67961165 0.70873786 0.90291262 0.73786408 0.75728155 0.75728155\n",
      " 0.75728155 0.94174757 0.66990291 0.7184466  0.7184466  0.7184466\n",
      " 0.93203883 0.9223301  0.9223301  0.9223301  0.9223301  0.65048544\n",
      " 0.91262136 0.83495146 0.81553398 0.80582524 0.74757282 0.9223301\n",
      " 0.88349515 0.89320388 0.89320388 0.82524272 0.86407767 0.77669903\n",
      " 0.78640777 0.78640777 0.70873786 0.95145631 0.93203883 0.91262136\n",
      " 0.88349515 0.93203883 0.87378641 0.7184466  0.69902913 0.70873786\n",
      " 0.98058252 0.68932039 0.63106796 0.63106796 0.63106796 0.95145631\n",
      " 0.76699029 0.66019417 0.65048544 0.65048544 0.68932039 0.93203883\n",
      " 0.9223301  0.80582524 0.7184466  0.99029126 0.89320388 0.72815534\n",
      " 0.70873786 0.7184466  0.91262136 0.7184466  0.75728155 0.75728155\n",
      " 0.75728155 0.94174757 0.66990291 0.7184466  0.7184466  0.7184466\n",
      " 0.93203883 0.9223301  0.9223301  0.9223301  0.9223301  0.65048544\n",
      " 0.91262136 0.83495146 0.81553398 0.80582524 0.74757282 0.9223301\n",
      " 0.88349515 0.83495146 0.78640777 0.7961165  0.86407767 0.75728155\n",
      " 0.70873786 0.70873786 0.70873786 0.95145631 0.93203883 0.91262136\n",
      " 0.88349515 0.93203883 0.87378641 0.7184466  0.70873786 0.69902913\n",
      " 0.98058252 0.68932039 0.63106796 0.63106796 0.63106796 0.95145631\n",
      " 0.74757282 0.67961165 0.67961165 0.67961165 0.68932039 0.93203883\n",
      " 0.9223301  0.81553398 0.7184466  0.98058252 0.89320388 0.7184466\n",
      " 0.68932039 0.69902913 0.91262136 0.75728155 0.76699029 0.77669903\n",
      " 0.76699029 0.94174757 0.67961165 0.69902913 0.69902913 0.69902913\n",
      " 0.93203883 0.9223301  0.9223301  0.9223301  0.9223301  0.65048544\n",
      " 0.91262136 0.83495146 0.81553398 0.80582524 0.74757282 0.9223301\n",
      " 0.89320388 0.89320388 0.89320388 0.7961165  0.85436893 0.72815534\n",
      " 0.66019417 0.66019417 0.70873786 0.95145631 0.93203883 0.91262136\n",
      " 0.88349515 0.93203883 0.7961165  0.70873786 0.67961165 0.67961165\n",
      " 0.98058252 0.69902913 0.69902913 0.7184466  0.7184466  0.95145631\n",
      " 0.7184466  0.66019417 0.65048544 0.6407767  0.68932039 0.93203883\n",
      " 0.9223301  0.81553398 0.7184466  0.99029126 0.89320388 0.7184466\n",
      " 0.69902913 0.69902913 0.91262136 0.73786408 0.76699029 0.76699029\n",
      " 0.76699029 0.94174757 0.66990291 0.70873786 0.70873786 0.70873786]\n",
      "best_score (Split_2) = 0.9902912621359223\n",
      "\n",
      "Split_3 Scores\n",
      " [1.         0.83495146 0.86407767 0.86407767 0.86407767 0.65048544\n",
      " 0.90291262 0.96116505 0.97087379 0.96116505 0.84466019 0.7961165\n",
      " 0.82524272 0.83495146 0.86407767 0.96116505 0.93203883 0.93203883\n",
      " 0.93203883 0.9223301  0.60194175 0.77669903 0.84466019 0.88349515\n",
      " 0.93203883 0.88349515 0.9223301  0.93203883 0.97087379 0.95145631\n",
      " 0.97087379 0.9223301  0.90291262 0.87378641 0.87378641 0.80582524\n",
      " 0.94174757 0.93203883 0.93203883 0.9223301  0.7961165  0.84466019\n",
      " 0.89320388 0.91262136 0.91262136 0.73786408 0.80582524 0.81553398\n",
      " 0.81553398 0.81553398 0.68932039 0.85436893 0.87378641 0.85436893\n",
      " 0.85436893 0.70873786 0.85436893 0.83495146 0.83495146 0.83495146\n",
      " 1.         0.83495146 0.86407767 0.86407767 0.86407767 0.6407767\n",
      " 0.86407767 0.99029126 0.97087379 0.97087379 0.84466019 0.77669903\n",
      " 0.77669903 0.80582524 0.81553398 0.99029126 0.94174757 0.96116505\n",
      " 0.96116505 0.96116505 0.60194175 0.76699029 0.83495146 0.86407767\n",
      " 0.93203883 0.87378641 0.91262136 0.93203883 0.94174757 0.94174757\n",
      " 0.97087379 0.87378641 0.85436893 0.81553398 0.81553398 0.7961165\n",
      " 0.9223301  0.95145631 0.93203883 0.93203883 0.7961165  0.82524272\n",
      " 0.88349515 0.90291262 0.91262136 0.73786408 0.80582524 0.81553398\n",
      " 0.81553398 0.83495146 0.69902913 0.82524272 0.82524272 0.83495146\n",
      " 0.83495146 0.68932039 0.9223301  0.85436893 0.85436893 0.85436893\n",
      " 1.         0.83495146 0.86407767 0.86407767 0.86407767 0.6407767\n",
      " 0.86407767 0.99029126 0.97087379 0.96116505 0.83495146 0.7961165\n",
      " 0.80582524 0.80582524 0.81553398 0.99029126 0.94174757 0.96116505\n",
      " 0.96116505 0.96116505 0.60194175 0.75728155 0.83495146 0.86407767\n",
      " 0.9223301  0.88349515 0.90291262 0.94174757 0.94174757 0.93203883\n",
      " 0.98058252 0.87378641 0.89320388 0.88349515 0.90291262 0.7961165\n",
      " 0.91262136 0.94174757 0.95145631 0.95145631 0.7961165  0.81553398\n",
      " 0.88349515 0.90291262 0.91262136 0.73786408 0.80582524 0.81553398\n",
      " 0.82524272 0.84466019 0.68932039 0.82524272 0.81553398 0.81553398\n",
      " 0.82524272 0.69902913 0.85436893 0.86407767 0.87378641 0.87378641\n",
      " 1.         0.83495146 0.86407767 0.86407767 0.86407767 0.6407767\n",
      " 0.86407767 0.99029126 0.97087379 0.96116505 0.83495146 0.78640777\n",
      " 0.78640777 0.80582524 0.81553398 0.99029126 0.93203883 0.96116505\n",
      " 0.96116505 0.96116505 0.60194175 0.75728155 0.83495146 0.86407767\n",
      " 0.9223301  0.88349515 0.91262136 0.90291262 0.95145631 0.93203883\n",
      " 0.97087379 0.87378641 0.89320388 0.89320388 0.89320388 0.78640777\n",
      " 0.9223301  0.96116505 0.93203883 0.93203883 0.7961165  0.81553398\n",
      " 0.88349515 0.90291262 0.91262136 0.73786408 0.80582524 0.82524272\n",
      " 0.81553398 0.83495146 0.69902913 0.83495146 0.81553398 0.81553398\n",
      " 0.81553398 0.69902913 0.85436893 0.83495146 0.87378641 0.85436893\n",
      " 1.         0.83495146 0.86407767 0.86407767 0.86407767 0.6407767\n",
      " 0.86407767 0.99029126 0.97087379 0.96116505 0.83495146 0.77669903\n",
      " 0.77669903 0.78640777 0.78640777 0.99029126 0.9223301  0.96116505\n",
      " 0.95145631 0.95145631 0.60194175 0.75728155 0.83495146 0.86407767\n",
      " 0.93203883 0.87378641 0.9223301  0.91262136 0.95145631 0.93203883\n",
      " 0.99029126 0.89320388 0.91262136 0.90291262 0.91262136 0.78640777\n",
      " 0.93203883 0.95145631 0.94174757 0.94174757 0.7961165  0.80582524\n",
      " 0.89320388 0.90291262 0.90291262 0.73786408 0.80582524 0.82524272\n",
      " 0.83495146 0.83495146 0.70873786 0.84466019 0.87378641 0.90291262\n",
      " 0.90291262 0.69902913 0.9223301  0.88349515 0.86407767 0.86407767]\n",
      "best_score (Split_3) = 1.0\n",
      "\n",
      "Split_4 Scores\n",
      " [0.44660194 0.49514563 0.51456311 0.51456311 0.51456311 0.40776699\n",
      " 0.51456311 0.61165049 0.67961165 0.72815534 0.52427184 0.5631068\n",
      " 0.66990291 0.67961165 0.66990291 0.32038835 0.5631068  0.65048544\n",
      " 0.6407767  0.6407767  0.42718447 0.39805825 0.46601942 0.54368932\n",
      " 0.59223301 0.44660194 0.61165049 0.63106796 0.66990291 0.69902913\n",
      " 0.36893204 0.65048544 0.72815534 0.75728155 0.74757282 0.38834951\n",
      " 0.65048544 0.7184466  0.68932039 0.68932039 0.34951456 0.46601942\n",
      " 0.53398058 0.59223301 0.62135922 0.37864078 0.63106796 0.69902913\n",
      " 0.7184466  0.7184466  0.4368932  0.72815534 0.72815534 0.72815534\n",
      " 0.72815534 0.46601942 0.72815534 0.72815534 0.73786408 0.73786408\n",
      " 0.44660194 0.49514563 0.51456311 0.51456311 0.51456311 0.40776699\n",
      " 0.51456311 0.61165049 0.67961165 0.70873786 0.52427184 0.58252427\n",
      " 0.6407767  0.67961165 0.70873786 0.32038835 0.58252427 0.65048544\n",
      " 0.66019417 0.67961165 0.42718447 0.38834951 0.46601942 0.54368932\n",
      " 0.59223301 0.45631068 0.62135922 0.66019417 0.67961165 0.69902913\n",
      " 0.36893204 0.65048544 0.70873786 0.74757282 0.72815534 0.39805825\n",
      " 0.69902913 0.73786408 0.68932039 0.68932039 0.34951456 0.46601942\n",
      " 0.53398058 0.59223301 0.61165049 0.37864078 0.63106796 0.68932039\n",
      " 0.70873786 0.69902913 0.42718447 0.72815534 0.74757282 0.74757282\n",
      " 0.74757282 0.46601942 0.74757282 0.73786408 0.73786408 0.73786408\n",
      " 0.44660194 0.49514563 0.51456311 0.51456311 0.51456311 0.40776699\n",
      " 0.51456311 0.61165049 0.67961165 0.70873786 0.52427184 0.57281553\n",
      " 0.6407767  0.66990291 0.7184466  0.32038835 0.62135922 0.66019417\n",
      " 0.66990291 0.69902913 0.42718447 0.38834951 0.47572816 0.54368932\n",
      " 0.59223301 0.45631068 0.62135922 0.67961165 0.7184466  0.7184466\n",
      " 0.36893204 0.65048544 0.72815534 0.72815534 0.75728155 0.39805825\n",
      " 0.70873786 0.73786408 0.7184466  0.66019417 0.34951456 0.46601942\n",
      " 0.53398058 0.59223301 0.61165049 0.37864078 0.63106796 0.68932039\n",
      " 0.70873786 0.69902913 0.42718447 0.73786408 0.73786408 0.73786408\n",
      " 0.73786408 0.46601942 0.69902913 0.73786408 0.73786408 0.72815534\n",
      " 0.44660194 0.49514563 0.51456311 0.51456311 0.51456311 0.40776699\n",
      " 0.51456311 0.61165049 0.67961165 0.70873786 0.52427184 0.5631068\n",
      " 0.63106796 0.66990291 0.7184466  0.32038835 0.62135922 0.66019417\n",
      " 0.66990291 0.70873786 0.42718447 0.38834951 0.47572816 0.54368932\n",
      " 0.59223301 0.45631068 0.63106796 0.66019417 0.69902913 0.70873786\n",
      " 0.36893204 0.65048544 0.69902913 0.73786408 0.73786408 0.38834951\n",
      " 0.68932039 0.72815534 0.67961165 0.68932039 0.34951456 0.46601942\n",
      " 0.53398058 0.58252427 0.61165049 0.37864078 0.63106796 0.66019417\n",
      " 0.69902913 0.69902913 0.41747573 0.73786408 0.73786408 0.74757282\n",
      " 0.74757282 0.45631068 0.67961165 0.73786408 0.7184466  0.7184466\n",
      " 0.44660194 0.49514563 0.51456311 0.51456311 0.51456311 0.40776699\n",
      " 0.51456311 0.61165049 0.67961165 0.70873786 0.52427184 0.51456311\n",
      " 0.63106796 0.63106796 0.66990291 0.32038835 0.62135922 0.66019417\n",
      " 0.66019417 0.70873786 0.42718447 0.38834951 0.47572816 0.54368932\n",
      " 0.59223301 0.45631068 0.62135922 0.65048544 0.70873786 0.7184466\n",
      " 0.36893204 0.65048544 0.66990291 0.73786408 0.76699029 0.38834951\n",
      " 0.69902913 0.7184466  0.67961165 0.6407767  0.34951456 0.46601942\n",
      " 0.53398058 0.59223301 0.61165049 0.37864078 0.63106796 0.67961165\n",
      " 0.69902913 0.69902913 0.41747573 0.73786408 0.73786408 0.72815534\n",
      " 0.72815534 0.45631068 0.74757282 0.77669903 0.75728155 0.75728155]\n",
      "best_score (Split_4) = 0.7766990291262136\n",
      "\n",
      "Split_5 Scores\n",
      " [0.61165049 0.73786408 0.76699029 0.76699029 0.76699029 0.48543689\n",
      " 0.76699029 0.7961165  0.75728155 0.76699029 0.54368932 0.73786408\n",
      " 0.74757282 0.77669903 0.77669903 0.46601942 0.76699029 0.78640777\n",
      " 0.80582524 0.80582524 0.74757282 0.74757282 0.75728155 0.76699029\n",
      " 0.77669903 0.66990291 0.81553398 0.7961165  0.80582524 0.7961165\n",
      " 0.54368932 0.83495146 0.82524272 0.82524272 0.82524272 0.54368932\n",
      " 0.85436893 0.84466019 0.83495146 0.83495146 0.54368932 0.7184466\n",
      " 0.76699029 0.75728155 0.76699029 0.57281553 0.76699029 0.78640777\n",
      " 0.80582524 0.81553398 0.72815534 0.73786408 0.77669903 0.76699029\n",
      " 0.76699029 0.76699029 0.76699029 0.80582524 0.80582524 0.80582524\n",
      " 0.61165049 0.7184466  0.75728155 0.75728155 0.75728155 0.48543689\n",
      " 0.77669903 0.78640777 0.75728155 0.75728155 0.52427184 0.73786408\n",
      " 0.75728155 0.78640777 0.78640777 0.4368932  0.75728155 0.72815534\n",
      " 0.80582524 0.81553398 0.75728155 0.75728155 0.75728155 0.77669903\n",
      " 0.77669903 0.66990291 0.81553398 0.81553398 0.83495146 0.83495146\n",
      " 0.53398058 0.81553398 0.80582524 0.83495146 0.83495146 0.53398058\n",
      " 0.78640777 0.75728155 0.74757282 0.74757282 0.54368932 0.72815534\n",
      " 0.76699029 0.74757282 0.76699029 0.57281553 0.75728155 0.7961165\n",
      " 0.81553398 0.82524272 0.75728155 0.7184466  0.75728155 0.75728155\n",
      " 0.76699029 0.75728155 0.76699029 0.76699029 0.76699029 0.76699029\n",
      " 0.61165049 0.7184466  0.75728155 0.74757282 0.74757282 0.48543689\n",
      " 0.77669903 0.7961165  0.75728155 0.75728155 0.50485437 0.72815534\n",
      " 0.75728155 0.78640777 0.78640777 0.4368932  0.74757282 0.77669903\n",
      " 0.7961165  0.7961165  0.75728155 0.75728155 0.75728155 0.75728155\n",
      " 0.75728155 0.66990291 0.81553398 0.82524272 0.83495146 0.83495146\n",
      " 0.53398058 0.83495146 0.80582524 0.82524272 0.83495146 0.53398058\n",
      " 0.78640777 0.74757282 0.73786408 0.73786408 0.54368932 0.72815534\n",
      " 0.77669903 0.74757282 0.76699029 0.57281553 0.75728155 0.7961165\n",
      " 0.81553398 0.81553398 0.74757282 0.7184466  0.74757282 0.74757282\n",
      " 0.74757282 0.72815534 0.73786408 0.76699029 0.77669903 0.77669903\n",
      " 0.61165049 0.7184466  0.75728155 0.74757282 0.74757282 0.48543689\n",
      " 0.77669903 0.7961165  0.76699029 0.75728155 0.50485437 0.73786408\n",
      " 0.75728155 0.7961165  0.78640777 0.4368932  0.75728155 0.76699029\n",
      " 0.77669903 0.77669903 0.75728155 0.75728155 0.75728155 0.75728155\n",
      " 0.75728155 0.66990291 0.82524272 0.80582524 0.84466019 0.83495146\n",
      " 0.50485437 0.83495146 0.82524272 0.80582524 0.80582524 0.53398058\n",
      " 0.76699029 0.72815534 0.73786408 0.75728155 0.54368932 0.72815534\n",
      " 0.77669903 0.74757282 0.76699029 0.57281553 0.75728155 0.7961165\n",
      " 0.81553398 0.81553398 0.74757282 0.70873786 0.75728155 0.77669903\n",
      " 0.77669903 0.74757282 0.76699029 0.74757282 0.75728155 0.75728155\n",
      " 0.61165049 0.7184466  0.75728155 0.74757282 0.74757282 0.48543689\n",
      " 0.77669903 0.7961165  0.76699029 0.75728155 0.51456311 0.73786408\n",
      " 0.76699029 0.78640777 0.78640777 0.45631068 0.75728155 0.76699029\n",
      " 0.77669903 0.80582524 0.75728155 0.75728155 0.75728155 0.75728155\n",
      " 0.75728155 0.66990291 0.82524272 0.85436893 0.86407767 0.85436893\n",
      " 0.54368932 0.84466019 0.81553398 0.81553398 0.81553398 0.51456311\n",
      " 0.76699029 0.75728155 0.74757282 0.74757282 0.54368932 0.72815534\n",
      " 0.77669903 0.74757282 0.76699029 0.57281553 0.75728155 0.7961165\n",
      " 0.80582524 0.81553398 0.73786408 0.7184466  0.74757282 0.75728155\n",
      " 0.75728155 0.75728155 0.75728155 0.7961165  0.80582524 0.80582524]\n",
      "best_score (Split_5) = 0.8640776699029126\n",
      "\n",
      "Split_6 Scores\n",
      " [0.59223301 0.63106796 0.69902913 0.70873786 0.70873786 0.58252427\n",
      " 0.7184466  0.73786408 0.74757282 0.77669903 0.58252427 0.68932039\n",
      " 0.72815534 0.7184466  0.70873786 0.58252427 0.7184466  0.7184466\n",
      " 0.75728155 0.76699029 0.6407767  0.65048544 0.65048544 0.73786408\n",
      " 0.7184466  0.62135922 0.7184466  0.69902913 0.73786408 0.75728155\n",
      " 0.58252427 0.7184466  0.72815534 0.74757282 0.74757282 0.6407767\n",
      " 0.74757282 0.78640777 0.80582524 0.80582524 0.60194175 0.72815534\n",
      " 0.73786408 0.75728155 0.75728155 0.63106796 0.72815534 0.75728155\n",
      " 0.81553398 0.81553398 0.65048544 0.7961165  0.81553398 0.80582524\n",
      " 0.78640777 0.63106796 0.70873786 0.74757282 0.74757282 0.74757282\n",
      " 0.59223301 0.63106796 0.68932039 0.70873786 0.70873786 0.58252427\n",
      " 0.70873786 0.74757282 0.72815534 0.73786408 0.58252427 0.68932039\n",
      " 0.72815534 0.7184466  0.69902913 0.54368932 0.72815534 0.73786408\n",
      " 0.77669903 0.76699029 0.6407767  0.65048544 0.65048544 0.72815534\n",
      " 0.72815534 0.62135922 0.7184466  0.70873786 0.76699029 0.78640777\n",
      " 0.58252427 0.7184466  0.72815534 0.73786408 0.76699029 0.6407767\n",
      " 0.73786408 0.73786408 0.76699029 0.76699029 0.60194175 0.72815534\n",
      " 0.73786408 0.74757282 0.75728155 0.63106796 0.72815534 0.78640777\n",
      " 0.78640777 0.7961165  0.65048544 0.82524272 0.82524272 0.83495146\n",
      " 0.83495146 0.63106796 0.77669903 0.77669903 0.77669903 0.77669903\n",
      " 0.59223301 0.63106796 0.68932039 0.70873786 0.70873786 0.58252427\n",
      " 0.70873786 0.74757282 0.72815534 0.73786408 0.58252427 0.68932039\n",
      " 0.74757282 0.70873786 0.69902913 0.54368932 0.72815534 0.72815534\n",
      " 0.77669903 0.76699029 0.6407767  0.65048544 0.65048544 0.72815534\n",
      " 0.72815534 0.62135922 0.7184466  0.7184466  0.76699029 0.77669903\n",
      " 0.59223301 0.7184466  0.72815534 0.73786408 0.77669903 0.6407767\n",
      " 0.73786408 0.75728155 0.75728155 0.75728155 0.60194175 0.72815534\n",
      " 0.73786408 0.74757282 0.75728155 0.63106796 0.72815534 0.76699029\n",
      " 0.7961165  0.80582524 0.65048544 0.82524272 0.81553398 0.81553398\n",
      " 0.81553398 0.63106796 0.77669903 0.77669903 0.77669903 0.77669903\n",
      " 0.59223301 0.63106796 0.68932039 0.70873786 0.70873786 0.58252427\n",
      " 0.70873786 0.74757282 0.72815534 0.73786408 0.58252427 0.68932039\n",
      " 0.73786408 0.70873786 0.68932039 0.54368932 0.72815534 0.73786408\n",
      " 0.77669903 0.77669903 0.6407767  0.65048544 0.65048544 0.72815534\n",
      " 0.72815534 0.62135922 0.7184466  0.70873786 0.76699029 0.77669903\n",
      " 0.58252427 0.70873786 0.72815534 0.74757282 0.78640777 0.6407767\n",
      " 0.73786408 0.75728155 0.76699029 0.82524272 0.60194175 0.72815534\n",
      " 0.73786408 0.74757282 0.75728155 0.63106796 0.72815534 0.76699029\n",
      " 0.7961165  0.7961165  0.65048544 0.82524272 0.82524272 0.84466019\n",
      " 0.84466019 0.63106796 0.77669903 0.77669903 0.74757282 0.74757282\n",
      " 0.59223301 0.63106796 0.68932039 0.70873786 0.70873786 0.58252427\n",
      " 0.70873786 0.74757282 0.72815534 0.73786408 0.58252427 0.69902913\n",
      " 0.73786408 0.70873786 0.76699029 0.55339806 0.72815534 0.73786408\n",
      " 0.77669903 0.76699029 0.6407767  0.65048544 0.65048544 0.72815534\n",
      " 0.72815534 0.62135922 0.7184466  0.7184466  0.76699029 0.77669903\n",
      " 0.58252427 0.7184466  0.74757282 0.75728155 0.75728155 0.6407767\n",
      " 0.73786408 0.73786408 0.72815534 0.72815534 0.60194175 0.72815534\n",
      " 0.73786408 0.74757282 0.75728155 0.63106796 0.72815534 0.77669903\n",
      " 0.80582524 0.81553398 0.65048544 0.82524272 0.80582524 0.82524272\n",
      " 0.82524272 0.63106796 0.77669903 0.7961165  0.7961165  0.7961165 ]\n",
      "best_score (Split_6) = 0.8446601941747572\n",
      "\n",
      "Split_7 Scores\n",
      " [0.78640777 0.9223301  0.9223301  0.9223301  0.9223301  0.74757282\n",
      " 0.93203883 0.9223301  0.9223301  0.90291262 0.58252427 0.93203883\n",
      " 0.91262136 0.87378641 0.86407767 0.75728155 0.93203883 0.91262136\n",
      " 0.90291262 0.90291262 0.91262136 0.90291262 0.94174757 0.93203883\n",
      " 0.93203883 0.83495146 0.89320388 0.84466019 0.82524272 0.83495146\n",
      " 0.86407767 0.91262136 0.83495146 0.84466019 0.84466019 0.90291262\n",
      " 0.89320388 0.89320388 0.90291262 0.90291262 0.91262136 0.94174757\n",
      " 0.94174757 0.93203883 0.88349515 0.89320388 0.90291262 0.86407767\n",
      " 0.86407767 0.89320388 0.91262136 0.96116505 0.9223301  0.90291262\n",
      " 0.90291262 0.91262136 0.78640777 0.80582524 0.80582524 0.80582524\n",
      " 0.78640777 0.9223301  0.93203883 0.94174757 0.95145631 0.74757282\n",
      " 0.95145631 0.93203883 0.93203883 0.93203883 0.61165049 0.9223301\n",
      " 0.9223301  0.88349515 0.86407767 0.81553398 0.91262136 0.90291262\n",
      " 0.89320388 0.89320388 0.91262136 0.91262136 0.93203883 0.93203883\n",
      " 0.93203883 0.83495146 0.90291262 0.82524272 0.82524272 0.82524272\n",
      " 0.86407767 0.84466019 0.84466019 0.84466019 0.84466019 0.90291262\n",
      " 0.85436893 0.86407767 0.87378641 0.87378641 0.91262136 0.94174757\n",
      " 0.94174757 0.90291262 0.87378641 0.89320388 0.91262136 0.84466019\n",
      " 0.7961165  0.83495146 0.91262136 0.9223301  0.9223301  0.9223301\n",
      " 0.9223301  0.90291262 0.77669903 0.73786408 0.73786408 0.73786408\n",
      " 0.78640777 0.9223301  0.9223301  0.93203883 0.93203883 0.74757282\n",
      " 0.95145631 0.93203883 0.94174757 0.93203883 0.62135922 0.93203883\n",
      " 0.93203883 0.86407767 0.86407767 0.7961165  0.91262136 0.88349515\n",
      " 0.85436893 0.86407767 0.91262136 0.91262136 0.93203883 0.93203883\n",
      " 0.93203883 0.83495146 0.90291262 0.84466019 0.84466019 0.85436893\n",
      " 0.86407767 0.86407767 0.81553398 0.83495146 0.86407767 0.90291262\n",
      " 0.89320388 0.85436893 0.85436893 0.85436893 0.91262136 0.94174757\n",
      " 0.94174757 0.89320388 0.87378641 0.89320388 0.91262136 0.84466019\n",
      " 0.81553398 0.87378641 0.91262136 0.94174757 0.9223301  0.9223301\n",
      " 0.9223301  0.90291262 0.76699029 0.73786408 0.73786408 0.73786408\n",
      " 0.78640777 0.9223301  0.9223301  0.93203883 0.93203883 0.74757282\n",
      " 0.95145631 0.93203883 0.94174757 0.93203883 0.62135922 0.93203883\n",
      " 0.93203883 0.86407767 0.86407767 0.82524272 0.88349515 0.85436893\n",
      " 0.84466019 0.84466019 0.91262136 0.91262136 0.93203883 0.93203883\n",
      " 0.93203883 0.83495146 0.90291262 0.84466019 0.82524272 0.82524272\n",
      " 0.86407767 0.85436893 0.84466019 0.86407767 0.86407767 0.90291262\n",
      " 0.89320388 0.85436893 0.85436893 0.85436893 0.91262136 0.94174757\n",
      " 0.94174757 0.89320388 0.88349515 0.89320388 0.91262136 0.84466019\n",
      " 0.82524272 0.84466019 0.91262136 0.90291262 0.90291262 0.90291262\n",
      " 0.90291262 0.90291262 0.76699029 0.7184466  0.7184466  0.7184466\n",
      " 0.78640777 0.9223301  0.9223301  0.94174757 0.94174757 0.74757282\n",
      " 0.95145631 0.9223301  0.94174757 0.93203883 0.62135922 0.93203883\n",
      " 0.93203883 0.86407767 0.87378641 0.84466019 0.91262136 0.91262136\n",
      " 0.87378641 0.88349515 0.91262136 0.91262136 0.93203883 0.93203883\n",
      " 0.93203883 0.83495146 0.90291262 0.83495146 0.82524272 0.84466019\n",
      " 0.85436893 0.87378641 0.86407767 0.87378641 0.87378641 0.90291262\n",
      " 0.89320388 0.85436893 0.85436893 0.84466019 0.91262136 0.94174757\n",
      " 0.94174757 0.89320388 0.88349515 0.89320388 0.9223301  0.83495146\n",
      " 0.84466019 0.87378641 0.9223301  0.91262136 0.91262136 0.91262136\n",
      " 0.91262136 0.90291262 0.75728155 0.75728155 0.75728155 0.75728155]\n",
      "best_score (Split_7) = 0.9611650485436893\n",
      "\n",
      "Split_8 Scores\n",
      " [0.70873786 0.69902913 0.72815534 0.72815534 0.72815534 0.52427184\n",
      " 0.69902913 0.73786408 0.74757282 0.78640777 0.60194175 0.73786408\n",
      " 0.77669903 0.80582524 0.81553398 0.65048544 0.7961165  0.80582524\n",
      " 0.7961165  0.7961165  0.55339806 0.61165049 0.67961165 0.70873786\n",
      " 0.70873786 0.65048544 0.7961165  0.7961165  0.7961165  0.77669903\n",
      " 0.62135922 0.81553398 0.84466019 0.82524272 0.83495146 0.62135922\n",
      " 0.78640777 0.73786408 0.72815534 0.72815534 0.66990291 0.68932039\n",
      " 0.72815534 0.73786408 0.73786408 0.66019417 0.70873786 0.80582524\n",
      " 0.86407767 0.86407767 0.66019417 0.82524272 0.84466019 0.84466019\n",
      " 0.84466019 0.66019417 0.78640777 0.76699029 0.76699029 0.76699029\n",
      " 0.70873786 0.69902913 0.74757282 0.75728155 0.75728155 0.52427184\n",
      " 0.68932039 0.73786408 0.75728155 0.76699029 0.61165049 0.73786408\n",
      " 0.76699029 0.78640777 0.78640777 0.6407767  0.73786408 0.76699029\n",
      " 0.82524272 0.82524272 0.55339806 0.61165049 0.69902913 0.7184466\n",
      " 0.74757282 0.65048544 0.76699029 0.73786408 0.69902913 0.72815534\n",
      " 0.62135922 0.7961165  0.77669903 0.80582524 0.81553398 0.6407767\n",
      " 0.82524272 0.85436893 0.83495146 0.81553398 0.66990291 0.68932039\n",
      " 0.72815534 0.72815534 0.70873786 0.66019417 0.72815534 0.75728155\n",
      " 0.81553398 0.84466019 0.66019417 0.82524272 0.85436893 0.84466019\n",
      " 0.83495146 0.65048544 0.72815534 0.74757282 0.74757282 0.74757282\n",
      " 0.70873786 0.69902913 0.74757282 0.75728155 0.75728155 0.52427184\n",
      " 0.68932039 0.73786408 0.75728155 0.76699029 0.61165049 0.74757282\n",
      " 0.77669903 0.78640777 0.78640777 0.6407767  0.74757282 0.83495146\n",
      " 0.85436893 0.85436893 0.55339806 0.61165049 0.69902913 0.7184466\n",
      " 0.74757282 0.65048544 0.76699029 0.74757282 0.7184466  0.73786408\n",
      " 0.62135922 0.78640777 0.76699029 0.81553398 0.82524272 0.6407767\n",
      " 0.83495146 0.84466019 0.84466019 0.83495146 0.66990291 0.68932039\n",
      " 0.72815534 0.7184466  0.69902913 0.66019417 0.72815534 0.75728155\n",
      " 0.80582524 0.84466019 0.66019417 0.7961165  0.85436893 0.88349515\n",
      " 0.88349515 0.65048544 0.69902913 0.7184466  0.74757282 0.74757282\n",
      " 0.70873786 0.69902913 0.74757282 0.75728155 0.75728155 0.52427184\n",
      " 0.68932039 0.73786408 0.75728155 0.76699029 0.61165049 0.74757282\n",
      " 0.77669903 0.76699029 0.78640777 0.63106796 0.76699029 0.81553398\n",
      " 0.81553398 0.81553398 0.55339806 0.60194175 0.69902913 0.7184466\n",
      " 0.74757282 0.65048544 0.74757282 0.73786408 0.7184466  0.73786408\n",
      " 0.62135922 0.7961165  0.7961165  0.81553398 0.81553398 0.63106796\n",
      " 0.7961165  0.83495146 0.81553398 0.83495146 0.66990291 0.68932039\n",
      " 0.72815534 0.72815534 0.70873786 0.66019417 0.72815534 0.75728155\n",
      " 0.80582524 0.84466019 0.66990291 0.85436893 0.83495146 0.83495146\n",
      " 0.82524272 0.65048544 0.70873786 0.72815534 0.72815534 0.72815534\n",
      " 0.70873786 0.69902913 0.74757282 0.75728155 0.75728155 0.52427184\n",
      " 0.68932039 0.73786408 0.75728155 0.76699029 0.61165049 0.74757282\n",
      " 0.77669903 0.80582524 0.80582524 0.63106796 0.77669903 0.78640777\n",
      " 0.83495146 0.84466019 0.55339806 0.60194175 0.69902913 0.7184466\n",
      " 0.74757282 0.65048544 0.77669903 0.72815534 0.70873786 0.74757282\n",
      " 0.63106796 0.82524272 0.81553398 0.84466019 0.83495146 0.63106796\n",
      " 0.82524272 0.83495146 0.81553398 0.81553398 0.66990291 0.68932039\n",
      " 0.72815534 0.72815534 0.70873786 0.66019417 0.72815534 0.75728155\n",
      " 0.81553398 0.85436893 0.66990291 0.80582524 0.84466019 0.84466019\n",
      " 0.84466019 0.65048544 0.69902913 0.72815534 0.72815534 0.72815534]\n",
      "best_score (Split_8) = 0.883495145631068\n",
      "\n",
      "Split_9 Scores\n",
      " [0.66990291 0.74757282 0.81553398 0.80582524 0.80582524 0.65048544\n",
      " 0.87378641 0.87378641 0.86407767 0.87378641 0.57281553 0.77669903\n",
      " 0.80582524 0.85436893 0.87378641 0.61165049 0.88349515 0.89320388\n",
      " 0.89320388 0.91262136 0.65048544 0.84466019 0.84466019 0.83495146\n",
      " 0.83495146 0.81553398 0.83495146 0.86407767 0.86407767 0.88349515\n",
      " 0.68932039 0.86407767 0.90291262 0.88349515 0.86407767 0.70873786\n",
      " 0.84466019 0.78640777 0.78640777 0.78640777 0.66990291 0.7961165\n",
      " 0.83495146 0.82524272 0.82524272 0.87378641 0.85436893 0.84466019\n",
      " 0.90291262 0.89320388 0.85436893 0.88349515 0.86407767 0.87378641\n",
      " 0.87378641 0.83495146 0.90291262 0.89320388 0.89320388 0.89320388\n",
      " 0.66990291 0.74757282 0.80582524 0.80582524 0.80582524 0.65048544\n",
      " 0.88349515 0.88349515 0.87378641 0.88349515 0.57281553 0.78640777\n",
      " 0.83495146 0.86407767 0.86407767 0.62135922 0.90291262 0.90291262\n",
      " 0.87378641 0.91262136 0.65048544 0.82524272 0.84466019 0.83495146\n",
      " 0.82524272 0.81553398 0.84466019 0.87378641 0.87378641 0.82524272\n",
      " 0.69902913 0.86407767 0.88349515 0.90291262 0.90291262 0.82524272\n",
      " 0.84466019 0.87378641 0.87378641 0.87378641 0.66019417 0.78640777\n",
      " 0.83495146 0.82524272 0.83495146 0.87378641 0.87378641 0.90291262\n",
      " 0.91262136 0.9223301  0.86407767 0.87378641 0.87378641 0.90291262\n",
      " 0.88349515 0.84466019 0.90291262 0.91262136 0.9223301  0.9223301\n",
      " 0.66990291 0.74757282 0.80582524 0.80582524 0.80582524 0.65048544\n",
      " 0.88349515 0.87378641 0.87378641 0.88349515 0.57281553 0.78640777\n",
      " 0.84466019 0.83495146 0.85436893 0.62135922 0.90291262 0.90291262\n",
      " 0.86407767 0.87378641 0.65048544 0.82524272 0.84466019 0.83495146\n",
      " 0.82524272 0.81553398 0.85436893 0.88349515 0.88349515 0.83495146\n",
      " 0.69902913 0.89320388 0.90291262 0.90291262 0.88349515 0.82524272\n",
      " 0.90291262 0.89320388 0.91262136 0.91262136 0.66019417 0.78640777\n",
      " 0.83495146 0.82524272 0.83495146 0.86407767 0.87378641 0.89320388\n",
      " 0.91262136 0.90291262 0.87378641 0.87378641 0.86407767 0.86407767\n",
      " 0.86407767 0.84466019 0.87378641 0.86407767 0.91262136 0.91262136\n",
      " 0.66990291 0.74757282 0.80582524 0.80582524 0.80582524 0.66019417\n",
      " 0.88349515 0.87378641 0.87378641 0.88349515 0.57281553 0.78640777\n",
      " 0.84466019 0.83495146 0.82524272 0.62135922 0.88349515 0.90291262\n",
      " 0.88349515 0.86407767 0.65048544 0.82524272 0.84466019 0.83495146\n",
      " 0.82524272 0.81553398 0.85436893 0.87378641 0.85436893 0.82524272\n",
      " 0.68932039 0.90291262 0.90291262 0.87378641 0.87378641 0.82524272\n",
      " 0.89320388 0.91262136 0.9223301  0.9223301  0.66019417 0.78640777\n",
      " 0.83495146 0.82524272 0.83495146 0.86407767 0.86407767 0.90291262\n",
      " 0.91262136 0.88349515 0.87378641 0.87378641 0.85436893 0.85436893\n",
      " 0.85436893 0.83495146 0.88349515 0.89320388 0.89320388 0.89320388\n",
      " 0.66990291 0.74757282 0.80582524 0.80582524 0.80582524 0.66019417\n",
      " 0.88349515 0.87378641 0.87378641 0.87378641 0.57281553 0.76699029\n",
      " 0.82524272 0.83495146 0.83495146 0.62135922 0.89320388 0.90291262\n",
      " 0.88349515 0.88349515 0.65048544 0.82524272 0.84466019 0.83495146\n",
      " 0.82524272 0.81553398 0.85436893 0.86407767 0.87378641 0.87378641\n",
      " 0.69902913 0.87378641 0.86407767 0.88349515 0.88349515 0.82524272\n",
      " 0.86407767 0.88349515 0.88349515 0.88349515 0.66019417 0.78640777\n",
      " 0.83495146 0.82524272 0.83495146 0.86407767 0.87378641 0.89320388\n",
      " 0.90291262 0.89320388 0.86407767 0.89320388 0.89320388 0.91262136\n",
      " 0.91262136 0.84466019 0.88349515 0.90291262 0.91262136 0.91262136]\n",
      "best_score (Split_9) = 0.9223300970873787\n",
      "\n",
      "Split_10 Scores\n",
      " [0.5631068  0.76699029 0.77669903 0.76699029 0.77669903 0.78640777\n",
      " 0.84466019 0.85436893 0.86407767 0.86407767 0.53398058 0.72815534\n",
      " 0.77669903 0.86407767 0.88349515 0.68932039 0.85436893 0.90291262\n",
      " 0.88349515 0.87378641 0.67961165 0.84466019 0.82524272 0.83495146\n",
      " 0.84466019 0.74757282 0.83495146 0.83495146 0.83495146 0.85436893\n",
      " 0.80582524 0.86407767 0.86407767 0.88349515 0.88349515 0.76699029\n",
      " 0.84466019 0.83495146 0.83495146 0.83495146 0.61165049 0.76699029\n",
      " 0.7961165  0.81553398 0.81553398 0.85436893 0.90291262 0.88349515\n",
      " 0.88349515 0.88349515 0.80582524 0.88349515 0.89320388 0.89320388\n",
      " 0.89320388 0.77669903 0.89320388 0.90291262 0.90291262 0.90291262\n",
      " 0.5631068  0.76699029 0.78640777 0.76699029 0.77669903 0.78640777\n",
      " 0.84466019 0.85436893 0.87378641 0.86407767 0.53398058 0.73786408\n",
      " 0.78640777 0.84466019 0.85436893 0.69902913 0.88349515 0.83495146\n",
      " 0.86407767 0.86407767 0.67961165 0.82524272 0.82524272 0.83495146\n",
      " 0.83495146 0.74757282 0.82524272 0.82524272 0.85436893 0.87378641\n",
      " 0.80582524 0.82524272 0.85436893 0.87378641 0.88349515 0.80582524\n",
      " 0.88349515 0.87378641 0.87378641 0.87378641 0.61165049 0.75728155\n",
      " 0.80582524 0.81553398 0.81553398 0.86407767 0.88349515 0.88349515\n",
      " 0.89320388 0.88349515 0.81553398 0.87378641 0.86407767 0.9223301\n",
      " 0.9223301  0.7961165  0.90291262 0.9223301  0.9223301  0.9223301\n",
      " 0.5631068  0.76699029 0.7961165  0.77669903 0.77669903 0.78640777\n",
      " 0.84466019 0.85436893 0.87378641 0.86407767 0.53398058 0.73786408\n",
      " 0.78640777 0.84466019 0.84466019 0.70873786 0.88349515 0.88349515\n",
      " 0.86407767 0.86407767 0.67961165 0.82524272 0.81553398 0.83495146\n",
      " 0.83495146 0.74757282 0.83495146 0.86407767 0.87378641 0.86407767\n",
      " 0.81553398 0.7961165  0.83495146 0.88349515 0.89320388 0.80582524\n",
      " 0.86407767 0.87378641 0.87378641 0.87378641 0.61165049 0.76699029\n",
      " 0.80582524 0.81553398 0.81553398 0.86407767 0.89320388 0.88349515\n",
      " 0.88349515 0.89320388 0.80582524 0.89320388 0.87378641 0.87378641\n",
      " 0.87378641 0.78640777 0.89320388 0.90291262 0.90291262 0.90291262\n",
      " 0.5631068  0.76699029 0.7961165  0.77669903 0.77669903 0.78640777\n",
      " 0.84466019 0.85436893 0.87378641 0.86407767 0.53398058 0.73786408\n",
      " 0.78640777 0.83495146 0.84466019 0.69902913 0.88349515 0.84466019\n",
      " 0.84466019 0.84466019 0.67961165 0.82524272 0.81553398 0.83495146\n",
      " 0.83495146 0.74757282 0.84466019 0.84466019 0.87378641 0.86407767\n",
      " 0.82524272 0.7961165  0.83495146 0.88349515 0.88349515 0.80582524\n",
      " 0.88349515 0.87378641 0.87378641 0.87378641 0.61165049 0.76699029\n",
      " 0.80582524 0.81553398 0.81553398 0.86407767 0.89320388 0.88349515\n",
      " 0.88349515 0.88349515 0.81553398 0.88349515 0.91262136 0.90291262\n",
      " 0.90291262 0.78640777 0.89320388 0.90291262 0.90291262 0.90291262\n",
      " 0.5631068  0.76699029 0.7961165  0.77669903 0.77669903 0.78640777\n",
      " 0.84466019 0.85436893 0.87378641 0.86407767 0.53398058 0.73786408\n",
      " 0.77669903 0.84466019 0.86407767 0.68932039 0.87378641 0.85436893\n",
      " 0.85436893 0.86407767 0.67961165 0.82524272 0.81553398 0.83495146\n",
      " 0.83495146 0.74757282 0.82524272 0.85436893 0.84466019 0.83495146\n",
      " 0.82524272 0.82524272 0.85436893 0.86407767 0.86407767 0.80582524\n",
      " 0.87378641 0.86407767 0.89320388 0.85436893 0.61165049 0.75728155\n",
      " 0.80582524 0.81553398 0.81553398 0.86407767 0.89320388 0.88349515\n",
      " 0.88349515 0.89320388 0.81553398 0.88349515 0.88349515 0.88349515\n",
      " 0.88349515 0.78640777 0.91262136 0.90291262 0.90291262 0.90291262]\n",
      "best_score (Split_10) = 0.9223300970873787\n"
     ]
    }
   ],
   "source": [
    "print('best_max_iterations =',best_max_iterations)\n",
    "print('best_alpha =',best_alpha)\n",
    "print('best_hidden_layers =', best_hidden_layers)\n",
    "print('Cross-Validation Mean Best Score for the Model =',best_score)\n",
    "print('\\nCross-Validation Mean Test Scores\\n', results['mean_test_score'])\n",
    "\n",
    "for i in range(10):\n",
    "  print('\\nSplit_'+str(i+1)+' Scores\\n',results['split'+str(i)+'_test_score'])\n",
    "  print('best_score (Split_'+str(i+1)+') =', max(results['split'+str(i)+'_test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMlR6zFwdACl"
   },
   "source": [
    "# **Data Fixed Single Splitting**\n",
    "(70% Training and 30% Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_PotaPq3k_tx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 8)\n",
      "           X1        X2        X3        X4        X5        X6        X7  \\\n",
      "185 -0.450228 -1.000000 -0.033483  0.078275 -0.720497 -0.034302  0.386352   \n",
      "286 -0.637443 -1.000000  0.669165 -0.236422 -0.527950  0.480233 -0.077772   \n",
      "600  0.082192 -1.000000 -1.000000  0.201278 -1.000000 -0.029070 -0.061716   \n",
      "691 -0.497717 -0.213689 -1.000000  0.305112 -1.000000  0.002326 -0.217260   \n",
      "\n",
      "           X8  \n",
      "185 -0.928571  \n",
      "286 -0.851648  \n",
      "600 -0.928571  \n",
      "691 -0.989011  \n",
      "(721,)\n",
      "(309, 8)\n",
      "           X1        X2        X3        X4        X5        X6        X7  \\\n",
      "339 -0.108676 -1.000000  0.174413 -0.153355 -0.409938  0.289535 -0.199699   \n",
      "244 -0.378539 -1.000000 -0.059470  0.036741 -0.565217 -0.134302  0.269443   \n",
      "882 -0.826484 -0.259878  0.029485  0.249201 -0.565217 -0.331395 -0.202208   \n",
      "567 -0.536530 -0.244853 -1.000000  0.020767 -1.000000  0.600000 -0.170597   \n",
      "\n",
      "           X8  \n",
      "339 -0.989011  \n",
      "244 -0.989011  \n",
      "882 -0.851648  \n",
      "567 -0.967033  \n",
      "(309,)\n"
     ]
    }
   ],
   "source": [
    "# Dividing samples dataset into training and test datasets:\n",
    "def dataset_divide(X, y):\n",
    "  X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.70, random_state=1)\n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = dataset_divide(X,y)\n",
    "print(X_train.shape)\n",
    "print(X_train.head(4))\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_test.head(4))\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z-23Sfvbk2y"
   },
   "source": [
    "# **Artificial Neural Network (ANN) Final Optimized Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2_5m5nNYbkl0"
   },
   "outputs": [],
   "source": [
    "# ANN Final Classification Model:\n",
    "def NN_Classification(X,y, max_iter=650, alpha=0.01, hidden_layer_sizes=[12, 12, 12]):\n",
    "  model = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation='relu',random_state=1, max_iter=max_iter,alpha=alpha)\n",
    "  model.fit(X, y)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-QBygtY9b0RN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of Model Evaluation with Testing Data = 0.8122977346278317\n",
      "rmse_test =  0.866492389746542\n",
      "pcc_test =  0.6256499198640039\n",
      "scc_test =  0.6256499198640041\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of Model Evaluation with Training Data = 0.9306518723994452\n",
      "rmse_train =  0.5266806531497233\n",
      "pcc_train =  0.8622728073107463\n",
      "scc_train =  0.8622728073107465\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANN Classification Model Training:\n",
    "NN_Model = NN_Classification(X_train, y_train)\n",
    "\n",
    "# NN Classification Model Evaluation on Testing Data:\n",
    "current_score = NN_Model.score(X_test,y_test)\n",
    "y_test_hat = NN_Model.predict(X_test) # testing output\n",
    "\n",
    "print('Score of Model Evaluation with Testing Data =', current_score)\n",
    "# RMSE\n",
    "rmse_test = sklearn.metrics.mean_squared_error(y_test, y_test_hat, squared=False)\n",
    "print('rmse_test = ',rmse_test)\n",
    "\n",
    "# Pearson's correlation\n",
    "pcc_test = scipy.stats.pearsonr(y_test, y_test_hat)[0]\n",
    "print ('pcc_test = ', pcc_test)\n",
    "\n",
    "#Spearman's correlation\n",
    "scc_test = scipy.stats.spearmanr(y_test, y_test_hat)[0]\n",
    "print ('scc_test = ', scc_test)\n",
    "\n",
    "matplotlib.pyplot.scatter(y_test,y_test_hat)\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "# NN Classification Model Evaluation on Training Data:\n",
    "current_score = NN_Model.score(X_train,y_train)\n",
    "y_train_hat = NN_Model.predict(X_train) # testing output\n",
    "\n",
    "print('Score of Model Evaluation with Training Data =', current_score)\n",
    "# RMSE\n",
    "rmse_train = sklearn.metrics.mean_squared_error(y_train, y_train_hat, squared=False)\n",
    "print('rmse_train = ',rmse_train)\n",
    "# Pearson's correlation\n",
    "pcc_train = scipy.stats.pearsonr(y_train, y_train_hat)[0]\n",
    "print ('pcc_train = ', pcc_train)\n",
    "\n",
    "#Spearman's correlation\n",
    "scc_train = scipy.stats.spearmanr(y_train, y_train_hat)[0]\n",
    "print ('scc_train = ', scc_train)\n",
    "\n",
    "matplotlib.pyplot.scatter(y_train,y_train_hat)\n",
    "matplotlib.pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
