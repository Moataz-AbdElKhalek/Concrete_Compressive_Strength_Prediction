# -*- coding: utf-8 -*-
"""RF_All.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qiEfwUEfVEO4pJxzDsuqvf3V_ZGd6FN4
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing relevant modules
import numpy
import pandas as pd
import matplotlib
# %matplotlib inline
import itertools
import IPython.display
from tqdm import tqdm_notebook as tqdm
import sklearn
import sklearn.model_selection
import sklearn.ensemble
import sklearn.model_selection

# Ignoring warnings
import warnings
warnings.filterwarnings("ignore")

# Load Dataset:
data_url = 'https://raw.githubusercontent.com/Moataz-AbdElKhalek/Concrete_Compressive_Strength_Prediction/main/dataset/Concrete_Dataset_Classification.csv'
dataset = pd.read_csv(data_url)

print(dataset.head(4))

# Descriptive statistics
print("\nDataset has {} rows and {} columns".format(dataset.shape[0],dataset.shape[1]))

print()
y = dataset['y']
print(y.head(4))
print(y.shape)
print()

X = dataset.drop(['y'], axis=1)
print(X.head(4))
print(X.shape)

# Applying statistical Analysis on the data:
dataset.describe()

# Dividing samples dataset into training and test datasets:
def dataset_divide(X, y):
  X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.70, random_state=1)
  return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = dataset_divide(X,y)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""# **Random Forest Model**
# Fixed Single Split
"""

def study_RF(max_depth,min_impurity):

    scores = numpy.zeros((len(min_impurity),len(max_depth)))
    
    for i in range (0,len(min_impurity)):
        for j in range (0,len(max_depth)):
            clf = sklearn.ensemble.RandomForestClassifier(max_depth=max_depth[j],min_impurity_decrease=min_impurity[i],random_state=1)
            clf.fit(X_train, y_train)
            clf.predict(X_test)
            scores[i,j]=clf.score(X_test,y_test)
    
    #best_min_impurity = numpy.unravel_index(numpy.argmin(grid, axis=None), grid.shape)[1]
    best_max_depth=max_depth[numpy.unravel_index(numpy.argmax(scores, axis=None), scores.shape)[1]]
    best_min_impurity=min_impurity[numpy.unravel_index(numpy.argmax(scores, axis=None), scores.shape)[0]]
    
    print('Max.score: ',numpy.amax(scores))
    print('best maximum depth: ',best_max_depth)
    print('best minimum impurity: ',best_min_impurity)
    print('----------scores----------')
    return scores

study_RF([4,8,12],[0.0,0.005,0.01])

"""# **Random Forest Model**
# 10-fold Cross-Validation
"""

def RFC_CV(max_depth_range,min_impurity_range):
    
  # Preparing the Model:
  model = sklearn.ensemble.RandomForestClassifier(criterion='gini', random_state=1)

  # Determining Model Hyperparameters to be tested and optimized:
  paras = {'max_depth':max_depth_range, 'min_impurity_decrease':min_impurity_range}
  # min_impurity_decrease is used instead of min_impurity_split as min_impurity_split is deprecated in favor of min_impurity_decrease.
  # And the official scikit-learn manual advises to use min_impurity_decrease.

  # Preparing Cross-Validation to be used to fit the Model and the Hyperparameters:
  # Using 10-fold Cross-Validation:
  gridCV = sklearn.model_selection.GridSearchCV(model, paras, cv=10, scoring='accuracy', verbose=10)
  gridCV.fit(X, y)

  best_max_depth = gridCV.best_params_['max_depth']
  best_min_impurity = gridCV.best_params_['min_impurity_decrease']
  best_score = gridCV.best_score_
  results = gridCV.cv_results_

  return best_max_depth, best_min_impurity, best_score, results

range(1,150,1),[0.0, 1e-10, 1e-5, 0.01, 0.1]

best_max_depth, best_min_impurity, best_score, cv_results = RFC_CV([1, 5, 10, 100, 1000],[0, 1e-10, 1e-5, 0.01, 0.1])

print('best_max_depth =',best_max_depth)
print('best_min_impurity =',best_min_impurity)
print('Cross-Validation Mean Best Score for the Model =',best_score)
print('\nCross-Validation Mean Test Scores\n', cv_results['mean_test_score'])

for i in range(10):
  print('\nSplit_'+str(i+1)+' Scores\n',cv_results['split'+str(i)+'_test_score'])
  print('best_score (Split_'+str(i+1)+') =', max(cv_results['split'+str(i)+'_test_score']))